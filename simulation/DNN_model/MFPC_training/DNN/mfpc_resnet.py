# -*- coding: utf-8 -*-
"""MFPC_ResNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yjEeQ8dA2IScIW0AtrGyS1IsJ8LwKPUF
"""

# Unzip data
!unzip "/content/drive/My Drive/Masters Year/data_mfpc/data_set_003.zip"

import cv2
import matplotlib
import numpy as np
import pandas as pd
from PIL import Image
import tensorflow as tf
from tensorflow import keras
import matplotlib.image as mpimg
from matplotlib import pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D
from tensorflow.keras.callbacks import EarlyStopping
np.random.seed(1000)
from sklearn import preprocessing
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import VGG19
from tensorflow.keras.applications import ResNet152V2
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet import preprocess_input

################################## Hyper Parameters ##################################################################

no_of_cameras = 1 + 1
data_set_length = 20

################################# Robot vel and pos ###################################################################
import imutils

for camera in range(1, no_of_cameras):
    robot_positions = pd.read_csv('/content/data_set_003/robot_pos/data_set_' + str(0) + '_robot_data_store_position.csv', header=None)
    robot_velocitys = pd.read_csv('/content/data_set_003/robot_vel/data_set_' + str(0) + '_robot_data_store_velocity.csv', header=None)
    for i in range(1, data_set_length):
        robot_positions = pd.concat([robot_positions, pd.read_csv('/content/data_set_003/robot_pos/data_set_' + str(i) + '_robot_data_store_position.csv', header=None)])
        robot_velocitys = pd.concat([robot_velocitys, pd.read_csv('/content/data_set_003/robot_vel/data_set_' + str(i) + '_robot_data_store_velocity.csv', header=None)])
    if camera == 1:
        robot_state = pd.concat([robot_positions, robot_velocitys], axis=1)
    else:
        robot_state_single_cam = pd.concat([robot_positions, robot_velocitys], axis=1)
        robot_state = pd.concat([robot_state, robot_state_single_cam], axis=0)

robot_states_list = robot_state.values.tolist()
list_ = []
for i in range(0, len(robot_state[0]), 4):
    list_.append(robot_states_list[i])
robot_states_frame_rate = pd.DataFrame(list_)
print("Done")

################################## Standardization for Robot States ###################################################################
robot_state_names = robot_states_frame_rate.columns
scaler = preprocessing.StandardScaler()
myScaler = scaler.fit(robot_states_frame_rate)
robot_states_frame_rate = myScaler.transform(robot_states_frame_rate)
robot_states_frame_rate = pd.DataFrame(robot_states_frame_rate, columns=robot_state_names)
print(robot_states_frame_rate.shape)

################################## Load Strawberry Data ##################################################################
strawberry_1 = pd.read_csv('/content/data_set_003/straw_1/data_set_' + str(0) + '_strawberry_data_store_1.csv', delimiter=',', error_bad_lines=False,  header=None)
strawberry_2 = pd.read_csv('/content/data_set_003/straw_2/data_set_' + str(0) + '_strawberry_data_store_2.csv', delimiter=',', error_bad_lines=False, header=None)
strawberry_3 = pd.read_csv('/content/data_set_003/straw_3/data_set_' + str(0) + '_strawberry_data_store_3.csv', delimiter=',', error_bad_lines=False, header=None)
strawberry_4 = pd.read_csv('/content/data_set_003/straw_4/data_set_' + str(0) + '_strawberry_data_store_4.csv', delimiter=',', error_bad_lines=False, header=None)
strawberry_5 = pd.read_csv('/content/data_set_003/straw_5/data_set_' + str(0) + '_strawberry_data_store_5.csv', delimiter=',', error_bad_lines=False, header=None)
strawberry_states = strawberry_1
blank = strawberry_2
counter = 0
for i in range(counter, 2):
    strawberry_states = pd.concat([strawberry_states, blank], axis=1)

for i in range(1, data_set_length):
    strawberry_cluster_state = pd.read_csv('/content/data_set_003/straw_1/data_set_' + str(i) + '_strawberry_data_store_1.csv', delimiter=',', error_bad_lines=False, header=None)
    strawberry_2 = pd.read_csv('/content/data_set_003/straw_2/data_set_' + str(i) + '_strawberry_data_store_2.csv', delimiter=',', error_bad_lines=False, header=None)
    strawberry_3 = pd.read_csv('/content/data_set_003/straw_3/data_set_' + str(i) + '_strawberry_data_store_3.csv', delimiter=',', error_bad_lines=False, header=None)
    strawberry_4 = pd.read_csv('/content/data_set_003/straw_4/data_set_' + str(i) + '_strawberry_data_store_4.csv', delimiter=',', error_bad_lines=False, header=None)
    strawberry_5 = pd.read_csv('/content/data_set_003/straw_5/data_set_' + str(i) + '_strawberry_data_store_5.csv', delimiter=',', error_bad_lines=False, header=None)
    counter = 0
    if strawberry_2[0][0] != 100 and counter < 2:
        strawberry_cluster_state = pd.concat([strawberry_cluster_state, strawberry_2], axis=1)
        counter += 1
    if strawberry_3[0][0] != 100 and counter < 2:
        strawberry_cluster_state = pd.concat([strawberry_cluster_state, strawberry_3], axis=1)
        counter += 1
    if strawberry_4[0][0] != 100 and counter < 2:
        strawberry_cluster_state = pd.concat([strawberry_cluster_state, strawberry_4], axis=1)
        counter += 1
    if strawberry_5[0][0] != 100 and counter < 2:
        strawberry_cluster_state = pd.concat([strawberry_cluster_state, strawberry_5], axis=1)
        counter += 1
    for i in range(counter, 2):
        strawberry_cluster_state = pd.concat([strawberry_cluster_state, blank], axis=1)
    strawberry_states = pd.concat([strawberry_states, strawberry_cluster_state], axis=0)

strawberry_states_list = strawberry_states.values.tolist()
list_ = []
for i in range(0, len(strawberry_states[0]), 4):
    list_.append(strawberry_states_list[i])
strawberry_states_frame_rate = pd.DataFrame(list_)

strawberry_states_frame_rate__ = strawberry_states_frame_rate
for camera in range(1, no_of_cameras-1):
    strawberry_states_frame_rate = pd.concat([strawberry_states_frame_rate, strawberry_states_frame_rate__], axis=0)

print("Done")

################################## Standardization for Strawberry States ###################################################################
strawberry_state_names = strawberry_states_frame_rate.columns
scaler = preprocessing.StandardScaler()
myScaler = scaler.fit(strawberry_states_frame_rate)
strawberry_states_frame_rate = myScaler.transform(strawberry_states_frame_rate)
strawberry_states_frame_rate = pd.DataFrame(strawberry_states_frame_rate, columns=strawberry_state_names)
print(strawberry_states_frame_rate.shape)

############################################### Load image data #####################################################
images = []
for camera in range(1, no_of_cameras):
    print("camera: ", camera)
    for i in range(0, data_set_length):
        for j in range(0, 50):
            im_frame = cv2.imread('/content/data_set_003/camera_' + str(camera) + '/rgb/sample_' + str(i) + '/time_step_' + str(j) + '.png')
            im_frame = imutils.resize(im_frame, width=224)
            # im_frame = tf.convert_to_tensor(im_frame)
            images.append(im_frame)
train_size = int((len(images) / 4) * 3)
test_size = len(images) - train_size

train_images = np.asarray(images[0:train_size])
train_images = train_images.astype(np.float64)
print("train_imagesing dataset size : {}".format(train_images.shape[0]))

test_images = np.asarray(images[train_size:train_size+test_size])
test_images = test_images.astype(np.float64)
print("test_imagesing dataset size : {}".format(test_images.shape[0]))

############################################### Normalize image data ###############################################
for i in range(0 , train_images.shape[0]):
    train_images[i, : , : , :] = train_images[i, : , : , :] / np.max(train_images[i, : , : , :])

robot_state_train_input = robot_states_frame_rate[0:train_images.shape[0]]
robot_pose_train_input = robot_state_train_input.iloc[:, 0:7]
robot_vel_train_input = robot_state_train_input.iloc[:, 7:14]
print("Robot pose input trainingset size: {}".format(robot_pose_train_input.shape))
print("Robot vel input trainingset size: {}".format(robot_vel_train_input.shape))
robot_state_train_label = strawberry_states_frame_rate[0:train_images.shape[0]]
print("Robot state label trainset size: {}".format(robot_state_train_label.shape))

for i in range(0 , test_images.shape[0]):
    test_images[i, : , : , :] = test_images[i, : , : , :] / np.max(test_images[i, : , : , :])

robot_state_test_input = robot_states_frame_rate[train_images.shape[0]:train_images.shape[0]+test_images.shape[0]]
robot_pose_test_input = robot_state_test_input.iloc[:, 0:7]
robot_vel_test_input = robot_state_test_input.iloc[:, 7:14]
print("Robot pose input trainingset size: {}".format(robot_pose_test_input.shape))
print("Robot vel input trainingset size: {}".format(robot_vel_test_input.shape))
robot_state_test_label = strawberry_states_frame_rate[train_images.shape[0]:train_images.shape[0]+test_images.shape[0]]
print("Robot state label testset size: {}".format(robot_state_test_label.shape))

###################################################################################################################################
model = ResNet152V2(include_top=False, weights='imagenet', input_shape=(224 , 224 , 3))

y1 = model.output
y2 = GlobalAveragePooling2D()(y1)
y3 = Dense(1024,activation='relu')(y2) 
y4 = Dense(1024,activation='relu')(y3)
new_model = Model(inputs=model.input,outputs=y4)

for layer in new_model.layers[:561]:
    layer.trainable=False
for layer in new_model.layers[561:]:
    layer.trainable=True
####################################################################################################################
cnn_out = new_model.output
robot_pose_input_layer = keras.layers.Input(shape=(7,))
dense_3_1 = keras.layers.Dense(15, activation=tf.keras.layers.LeakyReLU(alpha=0.3))(robot_pose_input_layer)
dense_4_1 = keras.layers.Dense(25, activation=tf.keras.layers.LeakyReLU(alpha=0.3))(dense_3_1)

robot_vel_input_layer = keras.layers.Input(shape=(7,))
dense_3_2 = keras.layers.Dense(15, activation=tf.keras.layers.LeakyReLU(alpha=0.1))(robot_vel_input_layer)
dense_4_2 = keras.layers.Dense(25, activation=tf.keras.layers.LeakyReLU(alpha=0.1))(dense_3_2)

concat = keras.layers.concatenate([dense_4_1, dense_4_2, cnn_out])

dense_5 = keras.layers.Dense(80, activation=tf.keras.layers.LeakyReLU(alpha=0.1))(concat)
dense_6 = keras.layers.Dense(30, activation=tf.keras.layers.LeakyReLU(alpha=0.1))(dense_5)
output_layer = keras.layers.Dense(21, activation="linear")(dense_6)

ResNet_model = keras.models.Model(inputs=[new_model.input, robot_pose_input_layer, robot_vel_input_layer] , outputs=output_layer)

ResNet_model.compile(optimizer='adam',loss='mean_absolute_error', metrics=['mse','accuracy'])

monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=2, verbose=1, mode='auto', restore_best_weights=True)

history = ResNet_model.fit([train_images, robot_pose_train_input, robot_vel_train_input], robot_state_train_label, callbacks=[monitor], batch_size=20, validation_split=0.2, epochs=15)
score = ResNet_model.evaluate([test_images, robot_pose_test_input, robot_vel_test_input] , robot_state_test_label)

predict_AlexNet_dense = ResNet_model.predict([test_images, robot_pose_test_input, robot_vel_test_input])

err_matrix_AlexNet_dense = robot_state_test_label - predict_AlexNet_dense
AlexNet_err_mean = np.mean(abs(err_matrix_AlexNet_dense))
print("AlexNet mean error values for each output: ")
print(AlexNet_err_mean)
a = np.where(err_matrix_AlexNet_dense > 0.01)
a = np.asarray(list(zip(*a)))
print("number of err elements higher than 0.01: {}".format(a.shape))

predict_AlexNet_dense.shape

ResNet_model.summary()
ResNet_model.save('/content/drive/My Drive/Masters Year/data_mfpc/ResNet_1000_leakyrelu_03.h5')

print(predict_AlexNet_dense[200])
print(robot_state_test_label.iloc[200])